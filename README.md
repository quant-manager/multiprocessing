# Multiprocessing: Reusable Code for Parallel Processing of Data Batches in Python

The objective of this project is to implement an efficient approach for processing big datasets. To achieve this objective, the implemented Python program uses batching, parallel processing, and efficient utilization of memory. The specific tasks within this objective are as follows: read data from two huge input text files, convert the data, perform computations with the converted data, do inverse conversion of the computational results, and ultimately save the results to a huge text file in the same format as the input files. Parallel processing across multiple processes was selected as a tool to achieve the objective. The implemented code was successfully tested on both Windows 11 PowerShell and WPS2 Ubuntu Linux environments.

As a prerequisite for the project objective, the two huge files have to be generated first. A separate script “generate_numbers.py” was implemented and run to produce such files. The script uses “ProcessPoolExecutor” from “concurrent.futures” package. The pool of parallel worker processes has an upper limit count. Each of these worker processes generates its own sub-sequence of random numbers with different seeds, and then saves this sub-sequence into a huge text file with the full sequence. The file access locks for either Windows or Linux systems are used by the parallel processes accordingly. Two huge text files with one billion integers in each can be generated with batches of size one million integers and with up to ten parallel worker processes at any moment. When the running CPU had eight Cores with two hardware threads per Core, it takes about 35 seconds to generate each file. Since the maximum random integer value is known (32767), the maximum number of decimal digits in all integers is known as well (5). Therefore, all integers under 10,000 are front-padded with zeros to assure their fixed length in decimal text representation.

After the files “hugefile1.txt” and “hugefile2.txt” are generated, another script “add_numbers.py” is used for processing them as inputs. Unlike “generate_numbers.py”, which uses “ProcessPoolExecutor”, “add_numbers.py” uses classes “JoinableQueue” and “Manager” from “multiprocessing” package. The big task of processing big files is split into more granular jobs, which are typically processed in parallel across processes assigned to different CPU Cores. Each job is responsible for one and only one batch, which is one of several disjoint subsets of the full task-related dataset. Each batch has approximately the same size. All batches from all jobs fully cover the entire dataset. The order of batches in the output file is non-deterministic, but this outcome is acceptable within the requirements of the project. The jobs are created in a blocking loop, and these jobs are inserted into the queue of type “JoinableQueue”. This FIFO queue has limited, predefined, and user provided maximum capacity. The predefined and fixed number of pre-initialized daemon processes retrieve jobs from this queue one at a time, process them, save the results to the same output file, and then wait in a blocking infinite loop for the next job from the queue.

The locking mechanism for this shared output writable file “totalfile.txt” is provided with the “fcntl” package on Linux and the “filelock” package on Windows. The input readable files do not require any locking for simultaneous access from multiple processes. Each job, while running in its pre-assigned worker process, spawns two threads for reading each input file in its own thread. This approach is more efficient than the sequential one. Each job produces a trace file with its own input batch data, but these trace files do not require any locking mechanism during their access.

When the main process exits, its daemon worker children-processes exit automatically as well, and this behavior is assured by their “daemon” status. However, a more graceful and correct approach is implemented in the code. When the main process stops generating new jobs for the queue, and the jobs’ queue is emptied by the worker processes, the main process is ready to exit. At this point, some worker processes may still be busy processing some remaining jobs and their batches. Therefore, the main process should not exit yet. Instead, before exiting, the main process puts several dummy job(s) with an “exit” indicator into the queue of jobs, and these daemon processes retrieve these dummy jobs to exit explicitly by themselves. The number of these inserted dummy jobs equals to the known number of worker processes. It means that each of these worker processes has a chance to complete its productive batch-related job, and then exit based on the signal from a dummy job. Only at this point, when all dummy jobs are consumed from the queue by the worker processes, the main process exits. As an extension for a better robustness, a timeout may be added to the “join” method invoked on an instance of “JoinableQueue” in the main process. This timeout would assure that the main process can eventually exit even when one or more of its worker processes were terminated prematurely and unexpectedly, or simply crashed.

After through testing of the above multiprocessing framework on input files of modest size, the testing phase shifted towards huge files as inputs. As a consequence, the “MemoryError” exception was produced. The issue was investigated, and the solution was found. Specifically, the use of Python “list” data structure and the use of Python “int” data type make the memory utilization very inefficient. First, the buffer for the “list” had to be implicitly reallocated many times. Second, the “int” data type supports an arbitrary magnitude of integers, and requires much more memory than needed. Therefore, the decision was made to switch to “uint16” data type from the NumPy library, and use memory-efficient NumPy arrays instead of native “lists”. This approach resolved the issue of “MemoryError” exception.

After through testing of the code on Windows 11 operating system, the testing of the same code was replicated in WSP2 Ubuntu Linux operating system. There was immediately a problem discovered, which can be explained with the following statement: “WSL 2 is only using up to 50% of the CPUs on my 8c/16t machine” (Renaux, 2019, June 14). In order to be able to run ten parallel processes, the file “C:\Users\<user name>\.wslconfig” had to be created with “[wsl2]” in the first line, and “processors=15” in the second line. Additionally, the virtual memory was increased in the third line with “memory=64GB” (Geronimo, 2023, November 24). This solution allowed running all ten processes in parallel, but not just eight.

After the above two challenges had been addressed, the final released code was used to generate the required results. One use case includes two jobs run in parallel in two processes. The jobs’ queue maximum size was to two as well. The checksums should be noted for the future reference and comparison. The total runtime was about 1,050 seconds. The output files and their line counts can be validated. The top and the bottom of the output file can be validated too. The fixed length of the output numbers is assured in the output files. This length is five decimal digits, which can fit both the maximum input integer (32767) and the maximum output integer (65534), which is produced as a sum of two maximum input integers.

Another use case includes ten jobs run in parallel in ten processes. There batch sizes are five times smaller than in the previous run with two jobs. The total runtime is only 587 seconds, which is a big improvement over 1,050 seconds. The grand total checksum was observed to be the same as the one in the previous use case. It means that the results are consistent across runs with different number of processes. The output files' line counts and sizes were validated and proveed to be as expected. The timestamps of the trace files showed that while the batch #0 was completed first, but batch #1 (not batch #9) was completed last. As was mentioned earlier, the order of batches in the output file is not guaranteed. In fact, while Figure 7.3.1 shows that the top of the “totalfile.txt” contains the results from batch #0, but the bottom of the “totalfile.txt” contains the results from batch #1 (not batch #9). This result is in compliance with the requirements of the project.

During project design and development, different methods of optimizing large file processing were observed. The first rule is that memory fragmentation should be avoided by allocating large static buffers, such as the ones from the NumPy arrays, instead of implicitly reallocating dynamic buffers, such as the ones in Python “list” type. The second rule is that memory should be used efficiently by using the proper data types, such as NumPy’s “uint16” type, instead of Python built-in class-type “int”, which takes many more bytes of memory than needed. The third rule is that the environment settings, such as virtual memory limit and the number of accessible CPU Cores, have to be evaluated and reset to the required values. The fourth rule is that the computational efficiency can be achieved by splitting the big task into smaller jobs, and splitting the big dataset into smaller batches. These jobs can process these batches in processes that are running in parallel on different Cores, while the final results can be assembled back into one large output dataset, or file. The fifth rule is that shared resources, such as the output file, must have locks activated before each point in the code that updates this output file. These locks must be immediately released after the updating operation is over. The sixth rule is that because large files require a lot of processing, this processing should better be done in a more efficient compiled C code, while the Python application can invoke these special C modules. This sixth rule was out of scope for this project, but it can be considered as a future extension.


# References

1. AML, (n.d.). Ensuring Data Integrity: Implementing File Locking in Python. Adventures in Machine Learning. Retrieved on 2024, October 18 from https://www.adventuresinmachinelearning.com/ensuring-data-integrity-implementing-file-locking-in-python
2. Community, (2017, May 23). Python Multiple users append to the same file at the same time. Stack Overflow. https://stackoverflow.com/questions/11853551/python-multiple-users-append-to-the-same-file-at-the-same-time
3. Darke, C. (2015, May 26). How to perform file-locking on Windows without installing a new package. Stack Overflow. https://stackoverflow.com/questions/30440559/how-to-perform-file-locking-on-windows-without-installing-a-new-package
4. Geronimo, (2023, November 24). Optimizing WSL2 Performance: Setting Memory and CPU Allocation on Windows. Medium. https://geronimo-bergk.medium.com/optimizing-wsl2-performance-setting-memory-and-cpu-allocation-on-windows-513eba7b6086
5. Pieters, M. (2018, October 18). How to use a multiprocessing.Manager()? Stack Overflow. https://stackoverflow.com/questions/9436757/how-to-use-a-multiprocessing-manager
6. Python, (2024, October 18). fcntl - The fcntl and ioctl system calls. Python 3.13.0 Documentation. https://docs.python.org/3/library/fcntl.html
7. Python, (2024, October 18). multiprocessing - Process-based parallelism. Python 3.13.0 Documentation. https://docs.python.org/3/library/multiprocessing.html
8. Renaux, P. (2019, June 14). WSL 2 only using half the cores of the host machine. Microsoft. Issues. GitHub. https://github.com/microsoft/WSL/issues/4137
9. Tutorials Point (n.d.). Processes Intercommunication. Tutorials Point. Retrieved on 2024, October 18 from https://www.tutorialspoint.com/concurrency_in_python/concurrency_in_python_processes_intercommunication.htm
